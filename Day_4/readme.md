# Day 4 (Agent Quality)

This whitepaper addresses the challenge of assuring quality in Al agents by introducing a holistic evaluation framework. The necessary technical foundation for this is Observability, built on three pillars: Logs (the diary), Traces (the narrative), and Metrics (the health report), enabling a continuous feedback loop using scalable methods like LLM-as-a-Judge and Human-in-the-Loop (HITL) evaluation.

For today's codelabs, you'll learn how to use logs, traces, and metrics to get full visibility into your agent's decision-making process, allowing you to debug failures and understand why your agent behaves the way it does. In the second codelab, you'll learn how to evaluate your agents to score your agent's response quality and tool usage.

## Course Materials
+ [summary podcast episode](https://www.youtube.com/watch?v=LFQRy-Ci-lk)
+ [Agent Quality Report](./resources/Agent%20Quality%20Report.pdf)
+ [DAY 4 Livestream](https://www.youtube.com/watch?v=JW1Yybfxyr4)

## Code
+ [Agent Observability](./code/Day%204a%20Agent%20Observability.ipynb)
+ [Agent Evaluation](./code/Day%204b%20Agent%20Evaluation.ipynb)

